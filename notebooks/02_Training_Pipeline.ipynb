{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Training Pipeline (End-to-End)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: ensure dependencies are installed (skip on Kaggle if already present)\n",
        "import sys, subprocess\n",
        "req = '../requirements.txt'\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '-r', req])\n",
        "except Exception as e:\n",
        "    print('Install skipped or failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "PROJECT_ROOT = Path('..')\n",
        "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'baseline_lgbm.yaml'\n",
        "TRAIN_PATH = PROJECT_ROOT / 'data' / 'train.csv'\n",
        "TEST_PATH = PROJECT_ROOT / 'data' / 'test.csv'\n",
        "\n",
        "with open(CONFIG_PATH, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading & Preprocessing\n",
        "import sys\n",
        "sys.path.append(str(PROJECT_ROOT / 'src'))\n",
        "\n",
        "from preprocess import load_and_clean_data, save_label_mapping\n",
        "\n",
        "train_df, label_map = load_and_clean_data(str(TRAIN_PATH), config)\n",
        "test_df, _ = load_and_clean_data(str(TEST_PATH), config)\n",
        "\n",
        "# Save label mapping for inference\n",
        "label_map_path = (PROJECT_ROOT / config['paths']['label_mapping_path'])\n",
        "label_map_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(label_map_path, 'w') as f:\n",
        "    json.dump(label_map, f, indent=2)\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering + Training (CV)\n",
        "from train import train_model\n",
        "\n",
        "oof_score, oof_df = train_model(train_df, config)\n",
        "print('OOF Macro F1:', oof_score)\n",
        "oof_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference on test and submission generation\n",
        "from predict import generate_predictions\n",
        "\n",
        "submission = generate_predictions(test_df, config['paths']['model_output_dir'], config)\n",
        "submission.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explainability (SHAP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstration with one fold model\n",
        "import joblib\n",
        "import shap\n",
        "from scipy import sparse\n",
        "from features import create_features, transform_tfidf\n",
        "\n",
        "model_dir = PROJECT_ROOT / config['paths']['model_output_dir']\n",
        "model_path = next(model_dir.glob('model_fold_*.pkl'))\n",
        "clf = joblib.load(model_path)\n",
        "fold = int(model_path.stem.split('_')[-1])\n",
        "\n",
        "# Build a small sample feature matrix for SHAP\n",
        "sample = test_df.sample(100, random_state=42) if len(test_df) > 100 else test_df.copy()\n",
        "sample_feat, art, feat_cols = create_features(sample, config=config, is_train=False, fold=fold)\n",
        "tfidf_cfg = config['features'].get('tfidf', {})\n",
        "tfidf_cols = tfidf_cfg.get('use_text_columns', [])\n",
        "tfidf_prefix = config['paths'].get('vectorizer_prefix')\n",
        "if len(tfidf_cols) > 0 and tfidf_prefix:\n",
        "    vectorizer_path = PROJECT_ROOT / f\"{tfidf_prefix}_{fold}.pkl\"\n",
        "    tfidf_matrix = transform_tfidf(sample_feat[tfidf_cols[0]], str(vectorizer_path))\n",
        "else:\n",
        "    tfidf_matrix = None\n",
        "\n",
        "from scipy import sparse as sp\n",
        "X_sample = sp.hstack([sp.csr_matrix(sample_feat[feat_cols].astype(float).fillna(0.0).values), tfidf_matrix], format='csr') if tfidf_matrix is not None else sp.csr_matrix(sample_feat[feat_cols].astype(float).fillna(0.0).values)\n",
        "explainer = shap.TreeExplainer(clf.booster_) if hasattr(clf, 'booster_') else shap.Explainer(clf)\n",
        "shap_values = explainer(X_sample[:50])\n",
        "shap.plots.beeswarm(shap_values, max_display=15)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DS_DA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
